{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Tweets\n",
    "\n",
    "We can gather a sample of Twitter data using the Twitter API (https://dev.twitter.com).  To do so, we'll need to create a Twitter application and get credentials for it.  You can do this manually at https://app.twitter.com.  Once you have an app, go to the \"Key and Access Tokens\" tab to find your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill these in!\n",
    "\n",
    "CONSUMER_KEY = ''\n",
    "CONSUMER_SECRET = ''\n",
    "ACCESS_TOKEN = ''\n",
    "ACCESS_SECRET = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "api = twitter.Twitter(\n",
    "    auth=twitter.OAuth(\n",
    "        consumer_key=CONSUMER_KEY,\n",
    "        consumer_secret=CONSUMER_SECRET,\n",
    "        token=ACCESS_TOKEN,\n",
    "        token_secret=ACCESS_SECRET,\n",
    "    ))\n",
    "\n",
    "stream = twitter.TwitterStream(\n",
    "    auth=twitter.OAuth(\n",
    "        consumer_key=CONSUMER_KEY,\n",
    "        consumer_secret=CONSUMER_SECRET,\n",
    "        token=ACCESS_TOKEN,\n",
    "        token_secret=ACCESS_SECRET,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.33 s, sys: 256 ms, total: 1.59 s\n",
      "Wall time: 21.5 s\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "%time st = list(itertools.islice(stream.statuses.sample(), 0, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 'Your reading Into it ....',\n",
       " 'RT @WTFFacts: Short term memory is the key to academic achievement.',\n",
       " '7/30„ÅÆÊó•ÊõúÊó•„ÅØÊú≠Âπå„Ç≤„Ç§ÊñáÈ§®„ÅßÁßÅ„Å®Êè°Êâã',\n",
       " 'RT @people: Driver Allegedly Claims He Didn‚Äôt Know He Was Smuggling People in Incident that Left 10 Dead https://t.co/p3zOyot4ai',\n",
       " 'Akira, cielo. ‚Äîqueesunani√±aunmomentoqueledaalgo‚Äî',\n",
       " 'With another White House delay, rule to bolster safety data on generic labels may be dead https://t.co/SsyEMlNKvG via @statnews',\n",
       " \"RT @rainnwilson: America is 5% of the worlds population but has 50% of the worlds guns. Cmon! Let's get that number up, people!\",\n",
       " 'Ben √ßocuklarƒ±ma babanƒ±zƒ± instagramdan buldum diyemem mesela',\n",
       " 'RT @fengzilin0312: #ÏÜ°Ï§ëÍ∏∞ ËÇ§ÊµÖÁöÑÊàë‰ªäÂ§©Â∞±ÊòØ‰∏ÄÊù°È¢úüê∂ https://t.co/gxxVeFeIn7']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t.get('text', None) for t in st][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nb_utils\n",
    "\n",
    "emotion_csv = nb_utils.download('https://www.crowdflower.com/wp-content/uploads/2016/07/text_emotion.csv')\n",
    "emotion_df = pd.read_csv(emotion_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>xoshayzers</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>coolfunky</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>czareaquino</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>xkilljoyx</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tweet_id   sentiment       author  \\\n",
       "0  1956967341       empty   xoshayzers   \n",
       "1  1956967666     sadness    wannamama   \n",
       "2  1956967696     sadness    coolfunky   \n",
       "3  1956967789  enthusiasm  czareaquino   \n",
       "4  1956968416     neutral    xkilljoyx   \n",
       "\n",
       "                                             content  \n",
       "0  @tiffanylue i know  i was listenin to bad habi...  \n",
       "1  Layin n bed with a headache  ughhhh...waitin o...  \n",
       "2                Funeral ceremony...gloomy friday...  \n",
       "3               wants to hang out with friends SOON!  \n",
       "4  @dannycastillo We want to trade with someone w...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutral       8638\n",
       "worry         8459\n",
       "happiness     5209\n",
       "sadness       5165\n",
       "love          3842\n",
       "surprise      2187\n",
       "fun           1776\n",
       "relief        1526\n",
       "hate          1323\n",
       "empty          827\n",
       "enthusiasm     759\n",
       "boredom        179\n",
       "anger          110\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying out a simple learner\n",
    "\n",
    "Before we try to build our deep learning models, let's make sure we can learn something using a simple linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/power/anaconda3/envs/perceptron/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "VOCAB_SIZE = 50000\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(max_features=VOCAB_SIZE)\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "linear_x = tfidf_vec.fit_transform(emotion_df['content'])\n",
    "linear_y = label_encoder.fit_transform(emotion_df['sentiment'])\n",
    "\n",
    "sgd = SGDClassifier(loss='hinge')\n",
    "bayes = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bayes': array([ 0.24615731,  0.28706412,  0.28954082]),\n",
       " 'sgd': array([ 0.29196971,  0.32943382,  0.320003  ])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{ \n",
    "    'sgd': cross_val_score(sgd, linear_x, linear_y),\n",
    "    'bayes': cross_val_score(bayes, linear_x, linear_y),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking what our model learned\n",
    "\n",
    "Our linear models appear to be learning something more powerful than \"pick the most popular category\".  We can take a quick look at which words they find the most correlated with each category before moving on to our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,\n",
       "       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
       "       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_sgd = sgd.fit(linear_x, linear_y)\n",
    "fitted_sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger ['rigging', 'transtelecom', 'aaaaaaaaaaa', 'fridaaaayyyyy', 'confuzzled']\n",
      "boredom ['cleanin', 'interminable', 'squeaking', 'meanmillies', 'documentation']\n",
      "empty ['bethsybsb', 'makinitrite', '_cheshire_cat_', 'conversating', 'kimbermuffin']\n",
      "enthusiasm ['krisswouldhowse', 'sotongs', 'foolproofdiva', 'lena_distractia', 'npyskater']\n",
      "fun ['yaaaaay', 'threee', 'yeaahh', 'tunes', 'bamboozle']\n",
      "happiness ['werewolfseth', 'wars', 'juddday', 'excellent', 'woohoo']\n",
      "hate ['cricinfo', 'bastard', 'zomberellamcfox', 'grrrr', 'hate']\n",
      "love ['loved', 'mommies', 'loving', 'mothers', 'love']\n",
      "neutral ['surfin', 'itchy', 'gut', 'frenchieb', 'mcraddictal']\n",
      "relief ['mastered', 'surviving', 'relaxed', 'relief', 'chiacy']\n",
      "sadness ['sadly', 'disappointed', 'depressing', 'cried', 'sad']\n",
      "surprise ['suprisingly', 'titanite', 'surprised', 'himym', 'surprise']\n",
      "worry ['scared', 'nervous', 'poor', 'throat', 'worried']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "inverse_vocab = { v:k for (k,v) in tfidf_vec.vocabulary_.items() }\n",
    "\n",
    "for i, klass in enumerate(label_encoder.classes_):\n",
    "    sorted_coef = np.argsort(fitted_sgd.coef_[i])\n",
    "    print(klass, [inverse_vocab[j] for j in sorted_coef[-5:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a deep model\n",
    "\n",
    "Now that we've seen how well a simple linear model can do, let's see if we can do any better with a deep learning model.  In this case, we don't have an excessive amount of training data: this constrains the models we can train effectively: use too big of a model, and we'll end up overfitting our data.\n",
    "\n",
    "We'll use pre-trained word embeddings again to bootstrap our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurizing and preparing our data\n",
    "\n",
    "Just like we did when computing word embeddings, we want to featurize our data so we can classify it effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "VOCAB_SIZE = 50000\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(emotion_df['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This may take a while to load\n",
    "\n",
    "w2v, idf = nb_utils.load_w2v(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens = tokenizer.texts_to_sequences(emotion_df['content'])\n",
    "tokens = pad_sequences(tokens)\n",
    "labels = label_encoder.transform(emotion_df['sentiment'])\n",
    "\n",
    "training_count = int(0.9 * len(tokens))\n",
    "training_tokens, training_labels = tokens[:training_count], labels[:training_count]\n",
    "test_tokens, test_labels = tokens[training_count:], labels[training_count:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "message (InputLayer)             (None, None)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "message_vec/embedding (Embedding (None, None, 300)     15000000    message[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "masking_7 (Masking)              (None, None, 300)     0           message_vec/embedding[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "message_idf/embedding (Embedding (None, None, 1)       50000       message[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "combine_and_sum (Lambda)         (None, 300)           0           masking_7[0][0]                  \n",
      "                                                                   message_idf/embedding[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dense_16 (Dense)                 (None, 128)           38528       combine_and_sum[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_17 (Dense)                 (None, 13)            1677        dense_16[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 15,090,205\n",
      "Trainable params: 40,205\n",
      "Non-trainable params: 15,050,000\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import layers, models\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "def make_embedding(name, vocab_size, embedding_size, weights=None, mask_zero=True):\n",
    "    if weights is not None:\n",
    "        return layers.Embedding(mask_zero=mask_zero, input_dim=vocab_size, \n",
    "                                output_dim=weights.shape[1], \n",
    "                                weights=[weights], trainable=False, \n",
    "                                name='%s/embedding' % name)\n",
    "    else:\n",
    "        return layers.Embedding(mask_zero=mask_zero, input_dim=vocab_size, \n",
    "                                output_dim=embedding_size,\n",
    "                                name='%s/embedding' % name)\n",
    "\n",
    "def create_unigram_model(vocab_size, embedding_size=None, embedding_weights=None, idf_weights=None):\n",
    "    assert not (embedding_size is None and embedding_weights is None)\n",
    "    message = layers.Input(shape=(None,), dtype='int32', name='message')\n",
    "    \n",
    "    embedding = make_embedding('message_vec', vocab_size, embedding_size, embedding_weights)\n",
    "    idf = make_embedding('message_idf', vocab_size, embedding_size, idf_weights)\n",
    "\n",
    "    mask = layers.Masking(mask_value=0)\n",
    "    def _combine_and_sum(args):\n",
    "        [embedding, idf] = args\n",
    "        return K.sum(embedding * K.abs(idf), axis=1)\n",
    "\n",
    "    sum_layer = layers.Lambda(_combine_and_sum, name='combine_and_sum')\n",
    "    sum_msg = sum_layer([mask(embedding(message)), idf(message)])\n",
    "    fc1 = layers.Dense(units=128, activation='relu')(sum_msg)\n",
    "    categories = layers.Dense(units=len(label_encoder.classes_), activation='softmax')(fc1)\n",
    "    \n",
    "    model = models.Model(\n",
    "        inputs=[message],\n",
    "        outputs=categories,\n",
    "    )\n",
    "    \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "unigram_model = create_unigram_model(vocab_size=VOCAB_SIZE,\n",
    "                                     embedding_weights=w2v,\n",
    "                                     idf_weights=idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "36000/36000 [==============================] - 0s - loss: 1.6016 - acc: 0.4407     \n",
      "Epoch 2/20\n",
      "36000/36000 [==============================] - 0s - loss: 1.5800 - acc: 0.4498     \n",
      "Epoch 3/20\n",
      "36000/36000 [==============================] - 0s - loss: 1.5601 - acc: 0.4531     \n",
      "Epoch 4/20\n",
      "36000/36000 [==============================] - 0s - loss: 1.5408 - acc: 0.4596     \n",
      "Epoch 5/20\n",
      "36000/36000 [==============================] - 0s - loss: 1.5222 - acc: 0.4640     \n",
      "Epoch 6/20\n",
      "36000/36000 [==============================] - 0s - loss: 1.5036 - acc: 0.4731     \n",
      "Epoch 7/20\n",
      "36000/36000 [==============================] - 0s - loss: 1.4883 - acc: 0.4750     \n",
      "Epoch 8/20\n",
      "36000/36000 [==============================] - 0s - loss: 1.4712 - acc: 0.4835     \n",
      "Epoch 9/20\n",
      "36000/36000 [==============================] - 0s - loss: 1.4580 - acc: 0.4839     \n",
      "Epoch 10/20\n",
      "36000/36000 [==============================] - 0s - loss: 1.4425 - acc: 0.4899     \n",
      "Epoch 11/20\n",
      "36000/36000 [==============================] - 0s - loss: 1.4291 - acc: 0.4948     \n",
      "Epoch 12/20\n",
      "36000/36000 [==============================] - 0s - loss: 1.4163 - acc: 0.5032     \n",
      "Epoch 13/20\n",
      "36000/36000 [==============================] - 0s - loss: 1.4023 - acc: 0.5035     \n",
      "Epoch 14/20\n",
      "36000/36000 [==============================] - 0s - loss: 1.3910 - acc: 0.5091     \n",
      "Epoch 15/20\n",
      "36000/36000 [==============================] - 0s - loss: 1.3798 - acc: 0.5127     \n",
      "Epoch 16/20\n",
      "36000/36000 [==============================] - 0s - loss: 1.3687 - acc: 0.5177     \n",
      "Epoch 17/20\n",
      "36000/36000 [==============================] - 0s - loss: 1.3560 - acc: 0.5205     - ETA: 0s - loss: 1.3036 - \n",
      "Epoch 18/20\n",
      "36000/36000 [==============================] - 0s - loss: 1.3456 - acc: 0.5241     \n",
      "Epoch 19/20\n",
      "36000/36000 [==============================] - 0s - loss: 1.3359 - acc: 0.5289     \n",
      "Epoch 20/20\n",
      "36000/36000 [==============================] - 0s - loss: 1.3261 - acc: 0.5299     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc599833400>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_model.fit(training_tokens, training_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.8783021488189697, 0.29899999999999999]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_model.evaluate(test_tokens, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Embeddings\n",
    "\n",
    "It looks like our model with pre-trained embeddings isn't doing much better than the linear models.\n",
    "\n",
    "We can also try training a model \"from scratch\", and learn the word embeddings from our training data.  Note that we use a small embedding size here to speed up training and to try to avoid overfitting.\n",
    "\n",
    "Only training for 10 epochs stops the model while it is still improving on the training set, but prevents it\n",
    "from overfitting.  We can formalize this by using a validation set and early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "message (InputLayer)             (None, None)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "message_vec/embedding (Embedding (None, None, 25)      1250000     message[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "masking_8 (Masking)              (None, None, 25)      0           message_vec/embedding[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "message_idf/embedding (Embedding (None, None, 25)      1250000     message[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "combine_and_sum (Lambda)         (None, 25)            0           masking_8[0][0]                  \n",
      "                                                                   message_idf/embedding[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dense_18 (Dense)                 (None, 128)           3328        combine_and_sum[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_19 (Dense)                 (None, 13)            1677        dense_18[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 2,505,005\n",
      "Trainable params: 2,505,005\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "learned_embeddings_model = create_unigram_model(vocab_size=VOCAB_SIZE, embedding_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "36000/36000 [==============================] - 2s - loss: 1.8780 - acc: 0.3591     \n",
      "Epoch 2/10\n",
      "36000/36000 [==============================] - 2s - loss: 1.8256 - acc: 0.3804     \n",
      "Epoch 3/10\n",
      "36000/36000 [==============================] - 2s - loss: 1.7770 - acc: 0.3977     \n",
      "Epoch 4/10\n",
      "36000/36000 [==============================] - 1s - loss: 1.7302 - acc: 0.4154     \n",
      "Epoch 5/10\n",
      "36000/36000 [==============================] - 1s - loss: 1.6818 - acc: 0.4373     \n",
      "Epoch 6/10\n",
      "36000/36000 [==============================] - 2s - loss: 1.6332 - acc: 0.4543     \n",
      "Epoch 7/10\n",
      "36000/36000 [==============================] - 2s - loss: 1.5827 - acc: 0.4713     \n",
      "Epoch 8/10\n",
      "36000/36000 [==============================] - 1s - loss: 1.5309 - acc: 0.4908     \n",
      "Epoch 9/10\n",
      "36000/36000 [==============================] - 1s - loss: 1.4776 - acc: 0.5110     \n",
      "Epoch 10/10\n",
      "36000/36000 [==============================] - 1s - loss: 1.4217 - acc: 0.5315     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc56477d160>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learned_embeddings_model.fit(training_tokens, training_labels, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0585128955841063, 0.35225000000000001]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the test set accuracy is lower than that on the training set.\n",
    "\n",
    "learned_embeddings_model.evaluate(test_tokens, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Complex Models\n",
    "\n",
    "As with our previous task, we can try using more powerful models to classify our text.  In this case, the limited training data and text size limit their effectiveness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_cnn_model(vocab_size, embedding_size=None, embedding_weights=None):\n",
    "    message = layers.Input(shape=(None,), dtype='int32', name='title')\n",
    "    \n",
    "    # The convolution layer in keras does not support masking, so we just allow\n",
    "    # the embedding layer to learn an explicit value.\n",
    "    embedding = make_embedding('message_vec', vocab_size, embedding_size, embedding_weights,\n",
    "                              mask_zero=False)\n",
    "\n",
    "    def _combine_sum(v):\n",
    "        return K.sum(v, axis=1)\n",
    "\n",
    "    cnn_1 = layers.Convolution1D(128, 3)\n",
    "    cnn_2 = layers.Convolution1D(128, 3)\n",
    "    cnn_3 = layers.Convolution1D(128, 3)\n",
    "    \n",
    "    global_pool = layers.GlobalMaxPooling1D()\n",
    "    local_pool = layers.MaxPooling1D(strides=1, pool_size=3)\n",
    "\n",
    "    cnn_encoding = global_pool(cnn_3(local_pool(cnn_2(local_pool(cnn_1(embedding(message)))))))\n",
    "    fc1 = layers.Dense(units=128, activation='elu')(cnn_encoding)\n",
    "    categories = layers.Dense(units=len(label_encoder.classes_), activation='softmax')(fc1)\n",
    "    model = models.Model(\n",
    "        inputs=[message],\n",
    "        outputs=[categories],\n",
    "    )\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "title (InputLayer)           (None, None)              0         \n",
      "_________________________________________________________________\n",
      "message_vec/embedding (Embed (None, None, 300)         15000000  \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, None, 128)         115328    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, None, 128)         49280     \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, None, 128)         49280     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 13)                1677      \n",
      "=================================================================\n",
      "Total params: 15,232,077\n",
      "Trainable params: 232,077\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model = create_cnn_model(VOCAB_SIZE, embedding_weights=w2v)\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "36000/36000 [==============================] - 8s - loss: 1.9750 - acc: 0.3151     \n",
      "Epoch 2/5\n",
      "36000/36000 [==============================] - 8s - loss: 1.8780 - acc: 0.3557     \n",
      "Epoch 3/5\n",
      "36000/36000 [==============================] - 8s - loss: 1.8227 - acc: 0.3714     \n",
      "Epoch 4/5\n",
      "36000/36000 [==============================] - 8s - loss: 1.7599 - acc: 0.3948     \n",
      "Epoch 5/5\n",
      "36000/36000 [==============================] - 8s - loss: 1.6880 - acc: 0.4200     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc4aa983ac8>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.fit(training_tokens, training_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3904/4000 [============================>.] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.9418595876693725, 0.35949999999999999]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.evaluate(test_tokens, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lstm_model(vocab_size, embedding_size=None, embedding_weights=None):\n",
    "    message = layers.Input(shape=(None,), dtype='int32', name='title')\n",
    "    embedding = make_embedding('message_vec', vocab_size, embedding_size, embedding_weights)(message)\n",
    "\n",
    "    lstm_1 = layers.LSTM(units=128, return_sequences=False)(embedding)\n",
    "#     lstm_2 = layers.LSTM(units=128, return_sequences=False)(lstm_1)\n",
    "    category = layers.Dense(units=len(label_encoder.classes_), activation='softmax')(lstm_1)\n",
    "    \n",
    "    model = models.Model(\n",
    "        inputs=[message],\n",
    "        outputs=[category],\n",
    "    )\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "title (InputLayer)           (None, None)              0         \n",
      "_________________________________________________________________\n",
      "message_vec/embedding (Embed (None, None, 300)         15000000  \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 13)                1677      \n",
      "=================================================================\n",
      "Total params: 15,221,325\n",
      "Trainable params: 221,325\n",
      "Non-trainable params: 15,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model = create_lstm_model(VOCAB_SIZE, embedding_weights=w2v)\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "36000/36000 [==============================] - 49s - loss: 2.0172 - acc: 0.3107    \n",
      "Epoch 2/5\n",
      "36000/36000 [==============================] - 49s - loss: 1.8888 - acc: 0.3531    \n",
      "Epoch 3/5\n",
      "36000/36000 [==============================] - 49s - loss: 1.8466 - acc: 0.3680    \n",
      "Epoch 4/5\n",
      "36000/36000 [==============================] - 49s - loss: 1.8158 - acc: 0.3768    \n",
      "Epoch 5/5\n",
      "36000/36000 [==============================] - 49s - loss: 1.7898 - acc: 0.3863    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc4af4563c8>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(training_tokens, training_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 7s     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.868003028869629, 0.38124999999999998]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.evaluate(test_tokens, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing our models\n",
    "\n",
    "Let's compare the predictions from our models on a sample of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {\n",
    "    'lstm': lstm_model.predict(test_tokens[:100]),\n",
    "    'cnn': cnn_model.predict(test_tokens[:100]),\n",
    "    'unigram': unigram_model.predict(test_tokens[:100]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>true</th>\n",
       "      <th>lstm</th>\n",
       "      <th>cnn</th>\n",
       "      <th>unigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HAPPY MOTHER'S DAY to all of the wonderful women out there.  Have a great and relaxful day.</td>\n",
       "      <td>happiness</td>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>browsing thru adopting agencies, i'm gonna get some exotic kids</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>neutral</td>\n",
       "      <td>fun</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am tired of my phone. Walkman works like a charm, but l need better video and wap really. Thanks for yesterday and for buy...</td>\n",
       "      <td>love</td>\n",
       "      <td>relief</td>\n",
       "      <td>happiness</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Happy Mother's Day to all the Mommiessss</td>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@mattgarner haha what's up Matt ?</td>\n",
       "      <td>happiness</td>\n",
       "      <td>neutral</td>\n",
       "      <td>happiness</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What's up!!? @guillermop</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@KandyBee we shuld do  a dance like that its seriously the best thing haha. see yu tomoro.</td>\n",
       "      <td>fun</td>\n",
       "      <td>happiness</td>\n",
       "      <td>happiness</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@TravelTweetie I will go to sleep now. Might be awakened early w/breakfast tray from my 'spark' &amp;amp; my 'joper' w/their Dad...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>worry</td>\n",
       "      <td>neutral</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@nak1a &amp;quot;If there's a camel up a hill&amp;quot; and &amp;quot;I'll give you plankton&amp;quot; ....HILARIOUS!!</td>\n",
       "      <td>happiness</td>\n",
       "      <td>happiness</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@Bern_morley LOL I love your kids</td>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                           content  \\\n",
       "0                                      HAPPY MOTHER'S DAY to all of the wonderful women out there.  Have a great and relaxful day.   \n",
       "1                                                                  browsing thru adopting agencies, i'm gonna get some exotic kids   \n",
       "2  I am tired of my phone. Walkman works like a charm, but l need better video and wap really. Thanks for yesterday and for buy...   \n",
       "3                                                                                         Happy Mother's Day to all the Mommiessss   \n",
       "4                                                                                                @mattgarner haha what's up Matt ?   \n",
       "5                                                                                                         What's up!!? @guillermop   \n",
       "6                                       @KandyBee we shuld do  a dance like that its seriously the best thing haha. see yu tomoro.   \n",
       "7  @TravelTweetie I will go to sleep now. Might be awakened early w/breakfast tray from my 'spark' &amp; my 'joper' w/their Dad...   \n",
       "8                           @nak1a &quot;If there's a camel up a hill&quot; and &quot;I'll give you plankton&quot; ....HILARIOUS!!   \n",
       "9                                                                                                @Bern_morley LOL I love your kids   \n",
       "\n",
       "         true       lstm        cnn    unigram  \n",
       "0   happiness       love       love       love  \n",
       "1  enthusiasm    neutral        fun  happiness  \n",
       "2        love     relief  happiness       love  \n",
       "3        love       love       love       love  \n",
       "4   happiness    neutral  happiness    neutral  \n",
       "5     neutral    neutral    neutral    neutral  \n",
       "6         fun  happiness  happiness  happiness  \n",
       "7   happiness      worry    neutral      worry  \n",
       "8   happiness  happiness    neutral    neutral  \n",
       "9        love       love       love       love  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a dataframe just for test data\n",
    "\n",
    "pd.options.display.max_colwidth = 128\n",
    "test_df = emotion_df[training_count:training_count+100].reset_index()\n",
    "eval_df = pd.DataFrame({\n",
    "    'content': test_df['content'],\n",
    "    'true': test_df['sentiment'],\n",
    "    'lstm': [label_encoder.classes_[np.argmax(x)] for x in predictions['lstm']],\n",
    "    'cnn': [label_encoder.classes_[np.argmax(x)] for x in predictions['cnn']],\n",
    "    'unigram': [label_encoder.classes_[np.argmax(x)] for x in predictions['unigram']],\n",
    "})\n",
    "eval_df = eval_df[['content', 'true', 'lstm', 'cnn', 'unigram']]\n",
    "eval_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Evaluation\n",
    "\n",
    "We can examine some of our error cases by hand.  Often, the models tend to agree when they make mistakes, and that the mistakes aren't unreasonable: this task would be challenging even for a human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>true</th>\n",
       "      <th>lstm</th>\n",
       "      <th>cnn</th>\n",
       "      <th>unigram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HAPPY MOTHER'S DAY to all of the wonderful women out there.  Have a great and relaxful day.</td>\n",
       "      <td>happiness</td>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>browsing thru adopting agencies, i'm gonna get some exotic kids</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>neutral</td>\n",
       "      <td>fun</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am tired of my phone. Walkman works like a charm, but l need better video and wap really. Thanks for yesterday and for buy...</td>\n",
       "      <td>love</td>\n",
       "      <td>relief</td>\n",
       "      <td>happiness</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@mattgarner haha what's up Matt ?</td>\n",
       "      <td>happiness</td>\n",
       "      <td>neutral</td>\n",
       "      <td>happiness</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>@KandyBee we shuld do  a dance like that its seriously the best thing haha. see yu tomoro.</td>\n",
       "      <td>fun</td>\n",
       "      <td>happiness</td>\n",
       "      <td>happiness</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@TravelTweetie I will go to sleep now. Might be awakened early w/breakfast tray from my 'spark' &amp;amp; my 'joper' w/their Dad...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>worry</td>\n",
       "      <td>neutral</td>\n",
       "      <td>worry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>@davecandoit dude that honest to god happens to me all the time.. minus the trail mix.</td>\n",
       "      <td>sadness</td>\n",
       "      <td>neutral</td>\n",
       "      <td>surprise</td>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Happy Mother's Day to the tweetin' mamas  Nite tweeple!</td>\n",
       "      <td>worry</td>\n",
       "      <td>happiness</td>\n",
       "      <td>love</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>On my way home...then SLEEP! Seeing Amber Pacific tomorow with the besties</td>\n",
       "      <td>happiness</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@xoMusicLoverxo I'm using it in a story. I actually already wrote it but have to write the chapters before it.</td>\n",
       "      <td>relief</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                            content  \\\n",
       "0                                       HAPPY MOTHER'S DAY to all of the wonderful women out there.  Have a great and relaxful day.   \n",
       "1                                                                   browsing thru adopting agencies, i'm gonna get some exotic kids   \n",
       "2   I am tired of my phone. Walkman works like a charm, but l need better video and wap really. Thanks for yesterday and for buy...   \n",
       "4                                                                                                 @mattgarner haha what's up Matt ?   \n",
       "6                                        @KandyBee we shuld do  a dance like that its seriously the best thing haha. see yu tomoro.   \n",
       "7   @TravelTweetie I will go to sleep now. Might be awakened early w/breakfast tray from my 'spark' &amp; my 'joper' w/their Dad...   \n",
       "10                                           @davecandoit dude that honest to god happens to me all the time.. minus the trail mix.   \n",
       "12                                                                          Happy Mother's Day to the tweetin' mamas  Nite tweeple!   \n",
       "13                                                       On my way home...then SLEEP! Seeing Amber Pacific tomorow with the besties   \n",
       "14                   @xoMusicLoverxo I'm using it in a story. I actually already wrote it but have to write the chapters before it.   \n",
       "\n",
       "          true       lstm        cnn    unigram  \n",
       "0    happiness       love       love       love  \n",
       "1   enthusiasm    neutral        fun  happiness  \n",
       "2         love     relief  happiness       love  \n",
       "4    happiness    neutral  happiness    neutral  \n",
       "6          fun  happiness  happiness  happiness  \n",
       "7    happiness      worry    neutral      worry  \n",
       "10     sadness    neutral   surprise   surprise  \n",
       "12       worry  happiness       love    neutral  \n",
       "13   happiness    neutral    neutral    neutral  \n",
       "14      relief    neutral    neutral    neutral  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df[eval_df['lstm'] != eval_df['true']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:perceptron]",
   "language": "python",
   "name": "conda-env-perceptron-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
