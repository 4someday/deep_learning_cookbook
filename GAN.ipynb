{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "import keras.layers as L\n",
    "import keras.backend as K\n",
    "\n",
    "img_width = 512\n",
    "img_height = 512\n",
    "\n",
    "# Using https://github.com/ctmakro/hellotensor/blob/master/lets_gan_clean.py\n",
    "# as a basis\n",
    "\n",
    "def build_gen_network(seed_shape):\n",
    "    input = L.Input(shape=(seed_shape,))\n",
    "    reshaped = L.Reshape((1,1,seed_shape))(input)\n",
    "    def deconv(layer, num_filters, kernel_size, strides=4, normalize=True, padding='same'):\n",
    "        layer = L.Conv2DTranspose(\n",
    "            num_filters, kernel_size, \n",
    "            padding=padding,\n",
    "            strides=strides)(layer)\n",
    "#         print(K.int_shape(layer))\n",
    "        if normalize:\n",
    "            layer = L.BatchNormalization()(layer)\n",
    "            layer = L.LeakyReLU(0.2)(layer)\n",
    "        return layer\n",
    "\n",
    "    reshaped = deconv(reshaped, 256, 4, padding='valid', strides=1)\n",
    "    reshaped = deconv(reshaped, 128, 4, padding='same')\n",
    "    reshaped = deconv(reshaped, 64, 4, padding='same')\n",
    "    reshaped = deconv(reshaped, 32, 4, padding='same')\n",
    "    reshaped = deconv(reshaped, 16, 4, padding='same', strides=2)\n",
    "    reshaped = deconv(reshaped, 1, 4, normalize=False, strides=1)    \n",
    "    \n",
    "    return Model(input=[input], output=[reshaped])\n",
    "\n",
    "gen_network = build_gen_network(100)\n",
    "gen_network.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "gen_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_discriminator_network():\n",
    "    input = L.Input(shape=(img_width, img_height, 1))\n",
    "    def conv(layer, num_filters, kernel_size, strides, normalize=True, padding='same'):\n",
    "        layer = L.Conv2D(\n",
    "            num_filters, kernel_size, \n",
    "            padding=padding,\n",
    "            strides=strides)(layer)\n",
    "#        print(K.int_shape(layer))\n",
    "        if normalize:\n",
    "            layer = L.BatchNormalization()(layer)\n",
    "            layer = L.LeakyReLU(0.2)(layer)\n",
    "        return layer\n",
    "    \n",
    "    l = conv(input, 32, kernel_size=4, strides=2)\n",
    "    l = conv(l, 64, kernel_size=4, strides=2)\n",
    "    l = conv(l, 128, kernel_size=4, strides=2)\n",
    "    l = conv(l, 256, kernel_size=4, strides=2)\n",
    "    l = conv(l, 512, kernel_size=4, strides=2)\n",
    "    l = conv(l, 1024, kernel_size=4, strides=2)\n",
    "    l = L.Flatten()(l)\n",
    "    l = L.Dense(units=1, activation='sigmoid')(l)\n",
    "    \n",
    "    return Model(input=[input], output=[l])\n",
    "\n",
    "discriminator_network = build_discriminator_network()\n",
    "discriminator_network.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "discriminator_network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gan_input = L.Input(gen_network.input_shape[1:])\n",
    "GAN = Model(gan_input, \n",
    "            discriminator_network(gen_network(gan_input)))\n",
    "GAN.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "GAN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "def make_trainable(net, val):\n",
    "    net.trainable = val\n",
    "    for l in net.layers:\n",
    "        l.trainable = val\n",
    "\n",
    "d_loss = []\n",
    "g_loss = []\n",
    "\n",
    "for i in range(10):\n",
    "    print('\\r', i, flush=True, end='')\n",
    "    noise_gen = np.random.uniform(0, 1, size=(BATCH_SIZE, 100))\n",
    "    image_batch = all_images[np.random.randint(0, all_images.shape[0], size=BATCH_SIZE)]\n",
    "    \n",
    "    generated = gen_network.predict(noise_gen)\n",
    "    X = np.concatenate((image_batch, generated))\n",
    "    Y = np.zeros(2 * BATCH_SIZE)\n",
    "    Y[:BATCH_SIZE] = 1\n",
    "    make_trainable(discriminator_network, True)\n",
    "    d_loss.append(discriminator_network.train_on_batch(X, Y))\n",
    "    \n",
    "    noise_tr = np.random.uniform(0, 1, size=(BATCH_SIZE,100))\n",
    "    y2 = np.zeros(BATCH_SIZE)\n",
    "    y2[:] = 1\n",
    "    make_trainable(discriminator_network, False)\n",
    "    g_loss.append(GAN.train_on_batch(noise_tr, y2))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "conda-env-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
